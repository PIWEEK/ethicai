{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46270d1d-6778-4a1c-ac5e-969a995c1e39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niwinz/.pyenv/versions/3.10.11/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# model and X_test, y_test, etc. are defined by this script\n",
    "from credoai.datasets import fetch_credit_model\n",
    "\n",
    "# model and data are defined by this script\n",
    "X_test, y_test, sensitive_features_test, model = fetch_credit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6387f7a6-7153-403a-a94d-48f68d2e082a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2308     female\n",
       "22404      male\n",
       "23397      male\n",
       "25058    female\n",
       "2664       male\n",
       "          ...  \n",
       "3211       male\n",
       "9355       male\n",
       "28201      male\n",
       "19705      male\n",
       "28313      male\n",
       "Name: SEX, Length: 7500, dtype: category\n",
       "Categories (2, object): ['female', 'male']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitive_features_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d94fd531-f5a0-421f-9399-11f6c399c800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2308     0\n",
       "22404    0\n",
       "23397    0\n",
       "25058    0\n",
       "2664     1\n",
       "        ..\n",
       "3211     0\n",
       "9355     0\n",
       "28201    1\n",
       "19705    1\n",
       "28313    0\n",
       "Name: target, Length: 7500, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d26b76-9e7e-4f89-81e0-9ebdf8c5f63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from credoai.lens import Lens\n",
    "from credoai.artifacts import ClassificationModel, TabularData\n",
    "from credoai.evaluators import ModelFairness, Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c62a322b-53a0-403e-af40-7ef0380d9db7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-12 18:45:15,483 - lens - INFO - Evaluator ModelFairness added to pipeline. Sensitive feature: SEX\n",
      "2023-04-12 18:45:15,750 - lens - INFO - Evaluator Performance added to pipeline. \n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<credoai.lens.lens.Lens at 0x7ff676a3b6a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credo_model = ClassificationModel(name=\"credit_default_classifier\", model_like=model)\n",
    "credo_data = TabularData(\n",
    "    name=\"UCI-credit-default\",\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    sensitive_features=sensitive_features_test,\n",
    ")\n",
    "lens = Lens(model=credo_model, assessment_data=credo_data)\n",
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "lens.add(ModelFairness(metrics=metrics))\n",
    "lens.add(Performance(metrics=metrics))\n",
    "lens.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c953d8d-d0d4-4947-bbc2-4ad602142330",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 2 evaluators\n"
     ]
    }
   ],
   "source": [
    "results = lens.get_results()\n",
    "print(f\"Results for {len(results)} evaluators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10cc5772-c3d3-48dc-a182-34b8ae8996dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision_score</td>\n",
       "      <td>0.628081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall_score</td>\n",
       "      <td>0.360172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              type     value\n",
       "0  precision_score  0.628081\n",
       "1     recall_score  0.360172"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.940916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.639828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.059084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.360172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_label predicted_label     value\n",
       "0           0               0  0.940916\n",
       "1           1               0  0.639828\n",
       "2           0               1  0.059084\n",
       "3           1               1  0.360172"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "performance_results = lens.get_results(evaluator_name='Performance')[0]\n",
    "# first dataframe is overall metrics\n",
    "display(performance_results['results'][0])\n",
    "# second dataframe is the long form of the confusion matrix\n",
    "display(performance_results['results'][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
